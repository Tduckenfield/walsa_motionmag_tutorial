{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/Tduckenfield/wisa_motionmag_tutorial/blob/main/start_here.ipynb)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<figure>\n","<center>\n","<img src=\"https://github.com/Tduckenfield/wisa_motionmag_tutorial/blob/main/figs/Anfinogentov_mm_example.jpg?raw=true\">\n","<figcaption>[Figure 7 of Anfinogentov et al 2022](https://link.springer.com/article/10.1007/s11214-021-00869-w)</figcaption></center>\n","</figure>"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"prlJF-xB5S8C"},"source":["# Motion magnification\n","Motion magnification is a processing technique which amplifies small transverse quasi-periodic motions of contrasted features in image sequences: it acts as a **microscope for videos**.\n","\n","Consider the humble microscope: through amplifying the image of small objects, microscopes have allowed us to view the world of tiny things the human eye cannot see. They led to some of the greatest scientific breakthroughs in history - for example cellular biology, forensic science, the existence of bacteria + viruses etc. Can we do the same thing but to small spatial scales changing over time in a video?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":400},"executionInfo":{"elapsed":4055,"status":"ok","timestamp":1687384448415,"user":{"displayName":"Tim Duckenfield","userId":"13806276132819639191"},"user_tz":-60},"id":"JEL5m-Ro5Esz","outputId":"99aae59c-5f42-46e8-e8d2-af9b66d4bbc8"},"outputs":[],"source":["%matplotlib inline\n","import numpy as np\n","from matplotlib import pyplot as plt\n","from matplotlib import animation\n","from IPython.display import HTML # To display video\n","from base64 import b64encode\n","#from google.colab import drive   # Connect to your google drive if you want to save anything\n","#drive.mount('/content/gdrive')\n","\n","!gdown 1l8HjifR4t52D16tx2NGtk0FoxxD3wWDp --output sergey.mp4\n","\n","# Show video\n","mp4 = open('/content/sergey.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=800 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ahlGX66Q86Kp"},"source":["This is [Dr Sergey Anfinogentov](https://orcid.org/0000-0002-1107-7420). He has provided everyone with a [code on Github](https://github.com/Sergey-Anfinogentov/motion_magnification) which can motion magnify any video. I aim to help you run motion magnification, and explain how it works!\n","\n","You can see in the video above, that whilst the video on the left looks quite static, there are a whole range of motions which are much more obvious on the right. **These motions were always there**.\n","\n","This processing tool has been used by me and others for use in solar physics, notably Sihui Zhong, Dong Li, Sudip Mandal, Yuhang Gao & Valery Nakariakov, all of whom are  here today! By magnifying the transverse motions of coronal structures seen in extreme ultraviolet, we have been able to study in great detail the regime of *decayless kink oscillations*.\n","\n","These waves are everywhere in the corona, so it is highly likely you could motion magnify any image sequence of the Sun where coronal structures are resolved (e.g. SDO/AIA 17.1 nm) and find some of these transverse waves. To prove me right/wrong, try going through the [mm_solardata](https://github.com/Tduckenfield/wisa_motionmag_tutorial/blob/main/notebooks/mm_solardata.ipynb) notebook and give it a go!   "]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"IN4xjXLnAo77"},"source":["# Working example\n","Let us do some motion magnification as a \"black box\" process. Feel free to use any video! Here I have picked a famous video of a guitar (which is often used in image processing), which you can access from my shared Google Drive - see the readme. \n","\n","I **strongly** suggest you take this opportunity and get your own video to motion magnify! Try to find something not too big, and with a relatively stable background. "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"background_save":true,"base_uri":"https://localhost:8080/","height":247,"output_embedded_package_id":"1K_Fs1AbBdIK_AuUDnZ6dTzrKgIjIerNw"},"executionInfo":{"elapsed":19399,"status":"ok","timestamp":1687385765498,"user":{"displayName":"Tim Duckenfield","userId":"13806276132819639191"},"user_tz":-60},"id":"vEEaGtK6BLn2","outputId":"4ebb17ee-8e0d-48fe-bdc5-b14ea25fd771"},"outputs":[],"source":["# Access guitar video. Replace with your own!\n","!gdown 1jK4K3hASDLyV1_IL8i7aHN0Bw_NmPY8Q --output guitar.mp4\n","\n","# Input video path\n","mymovie_path = \"/content/guitar.mp4\"\n","# For those on the Google drive, something like \"/content/gdrive/MyDrive/myvideo.mp4\"\n","\n","# Show video\n","mp4 = open(mymovie_path,'rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=400 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The following cell will convert our video to a numpy array."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"F6PeNEGyBeOd"},"outputs":[],"source":["import cv2\n","\n","videoReader = cv2.VideoCapture(mymovie_path)  # create video object\n","nFrames = int(videoReader.get(cv2.CAP_PROP_FRAME_COUNT))\n","frameWidth = int(videoReader.get(cv2.CAP_PROP_FRAME_WIDTH))\n","frameHeight = int(videoReader.get(cv2.CAP_PROP_FRAME_HEIGHT))\n","\n","print(\"Total Number of Frames = \", nFrames)\n","print(\"Each frame has dimensions = \",frameHeight,\"x\", frameWidth)\n","\n","# Note we specify float32 since necessary for cv2.color_bgr2gray\n","img_array_rgb = np.zeros((nFrames, frameHeight,frameWidth, 3), np.dtype(\"float32\"))\n","img_array = np.zeros((nFrames, frameHeight,frameWidth), np.dtype(\"float32\"))\n","\n","nf = 0\n","ret = True\n","\n","while (nf < nFrames  and ret):  # Could do for loop as well\n","#\t\tprint(\"frame: \" + str(nf))\n","    ret, img_array_rgb[nf] = videoReader.read()\n","    img_array[nf] = cv2.cvtColor(img_array_rgb[nf], cv2.COLOR_RGB2GRAY)\n","    nf += 1\n","#\t\tif ret == False or img is None:\n","#\t\t\t\tprint(\"done\")\n","#\t\t\t\tbreak\n","\n","#videoWriter = make_video_writer(videoReader, output_video_filename)\n","\n","videoReader.release()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Take a note in particular of the number of frames. Also, I generally aim for a datacube $\\le$ 500 x 1000 x 1000  to get the motion magnification to run without any problems. \n","\n","Now we install the necessary library and get the motion magnification code we will use. To learn more about the DTCWT library upon which this MM code is based, see the [understanding_dtcwt](https://github.com/Tduckenfield/wisa_motionmag_tutorial/blob/main/notebooks/understanding_dtwct.ipynb) notebook."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["%pip --quiet install dtcwt\n","!git clone https://github.com/Sergey-Anfinogentov/motion_magnification.git\n","%cd /content/motion_magnification\n","from magnify import *"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["And now do the magnification! We have to choose two parameters, more on that later. This may take a few minutes depending on the size of the datacube."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["k = 10 # Amplification factor\n","w = 50 # Width, motions who have a timescale < w will be amplified.\n","\n","img_mm = magnify_motions_2d(img_array, k, 20)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we save out the cube. If you know python, you could directly image the magnified data (as a numpy cube) at this point using `FuncAnimation` etc. I do this later, but in a very crude way.  \n","For now, let us just admire the magnified video."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from matplotlib import animation\n","\n","# Here I choose some standard mp4 parameters. Should adjust for data, e.g. 10 fps on 12s cadence AIA data is too slow!\n","output_path = '/content/mymovie_mm.mp4'\n","\n","Writer = animation.writers['ffmpeg']\n","writer = Writer(fps=10, metadata=dict(artist='Me'), bitrate=180)\n","\n","# Define a rough function to create animation. \n","def Create_Animation(image,number_of_files,title):\n","    ## Create an Array of pictures from a Data Cube\n","    images=[]\n","    fig=plt.figure()\n","    for i in range(number_of_files):\n","        img_plot=plt.imshow(image[i], cmap='gray')   # Output in greyscale\n","        images.append([img_plot])\n","    ani = animation.ArtistAnimation(fig, images, interval=100, blit=True)\n","    ani.save(title,writer=writer)\n","    return images\n","\n","# Create new video.\n","images=Create_Animation(img_mm,nFrames,output_path)\n","\n","# Show video\n","mp4 = open('/content/mymovie_mm.mp4','rb').read()\n","data_url = \"data:video/mp4;base64,\" + b64encode(mp4).decode()\n","HTML(\"\"\"\n","<video width=800 controls>\n","      <source src=\"%s\" type=\"video/mp4\">\n","</video>\n","\"\"\" % data_url)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["And there you have it! Hopefully you can see transverse motions have been amplified, revealing previously imperceptible phenomena. \n","\n","Within this github directory, there are some other notebooks focussing on this routine and its use in solar physics in more detail. In light of the themes for this conference - detecting vortex/swirls and identifying MHD wave modes - I suggest you work through [understanding_dtcwt.ipynb](https://colab.research.google.com/github/Tduckenfield/wisa_motionmag_tutorial/blob/main/notebooks/understanding_dtcwt.ipynb). Peruse at your leisure, but I **highly** recommend you give motion magnifying a video of your own a go. It is perfectly possible to take video from your phone and magnify it! Check out [bye.ipynb](https://colab.research.google.com/github/Tduckenfield/wisa_motionmag_tutorial/blob/main/notebooks/bye.ipynb) to see what I mean!"]}],"metadata":{"colab":{"authorship_tag":"ABX9TyPE8ZpP+bamGFNshKuytpB7","name":"","version":""},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}
